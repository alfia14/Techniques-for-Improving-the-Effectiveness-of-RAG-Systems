{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Data Exploration:\n",
        "\n",
        "TechBlogs are HTML pages from a Wordpress Blog. They have a leading title, and then are often further divided into sections, which can be one or more paragraphs. The blogs contain a mix of regular text, code, and images.\n",
        "\n",
        "If we \"embed\" the text of each post, we get a floating-point vector with hundreds or even thousands of dimensions quantifying the \"meaning\" of each blog post. To visualize this, we can reduce the dimensionality of these vectors so each blog is now represented by a 3D point which we can easily plot. Note how the embeddings naturally cluster the blogs, and we've color coded the clusters. Orange might represent blogs in the realm of healthcare and life-sciences. Magenta might capture blogs within the realm of robotics, etc.\n"
      ],
      "metadata": {
        "id": "Yqtwq7kP2GdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data\n",
        "\n",
        "import math\n",
        "import os\n",
        "import json\n",
        "import asyncio\n",
        "import httpx\n",
        "import time\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "POSTS_PER_PAGE = 25  # using 100 can cause HTML response to be too long so that the text gets terminated\n",
        "MAX_PAGE = 8  # setting to 8 so that we get 200 pages total. Increase to download more articles\n",
        "\n",
        "def download(session, headers, wp, data_dir):\n",
        "    current_page = 1\n",
        "    download_complete = False\n",
        "    now = datetime.now()\n",
        "    start_timecode = f\"{now.year}{now.month}{now.day}{now.hour}{now.minute}{now.second}\"\n",
        "    padding_width = math.ceil(math.log(MAX_PAGE, 10))\n",
        "\n",
        "    print(f\"Downloading up to {MAX_PAGE * POSTS_PER_PAGE} posts...\")\n",
        "    while (not download_complete) and (\n",
        "        current_page <= MAX_PAGE\n",
        "    ):  # <= because pages are 1-indexed\n",
        "        response = session.get(\n",
        "            f\"https://{wp}/wp-json/wp/v2/posts?page={current_page}&per_page={POSTS_PER_PAGE}\",\n",
        "            headers=headers,\n",
        "        )\n",
        "        if response.status_code == 200:\n",
        "            response_json = response.json()\n",
        "            with open(\n",
        "                os.path.join(\n",
        "                    data_dir,\n",
        "                    f\"{start_timecode}_{str(current_page).zfill(padding_width)}.json\",\n",
        "                ),\n",
        "                \"w\",\n",
        "            ) as dump_file:\n",
        "                json.dump(response_json, dump_file)\n",
        "\n",
        "            print(f\"Page {current_page}. Downloaded {len(response_json)} posts\")\n",
        "\n",
        "            if len(response_json) < POSTS_PER_PAGE:\n",
        "                download_complete = True\n",
        "                print(\n",
        "                    f\"Downloaded all ({POSTS_PER_PAGE * (current_page - 1) + len(response_json)} posts)\"\n",
        "                )\n",
        "            else:\n",
        "                current_page += 1\n",
        "\n",
        "        else:\n",
        "            print(\n",
        "                f\"Download of page {current_page} failed with status code {response.status_code}. {response.text}\"\n",
        "            )\n",
        "            download_complete = True"
      ],
      "metadata": {
        "id": "RxOexoQJ2bL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = os.path.join(os.getcwd(), 'data', 'techblogs')"
      ],
      "metadata": {
        "id": "jwtU6pPP2jCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Organize the tech blogs\n",
        "\n",
        "file_list = [x for x in sorted(os.listdir(data_dir)) if '.json' in x]\n",
        "\n",
        "techblogs_dict = {}\n",
        "\n",
        "for i, filename in enumerate(file_list):\n",
        "    with open(os.path.join(data_dir, filename), 'r') as in_file:\n",
        "        data = json.load(in_file)\n",
        "    for item in data:\n",
        "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
        "        if not item['link'].startswith(\"https://developer.nvidia.com/blog\"): # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
        "            continue\n",
        "        document_title = item['title']['rendered']\n",
        "        document_url = item['link']\n",
        "        document_html = item['content']['rendered']\n",
        "        document_date = item['date_gmt']\n",
        "        document_date_modified = item['modified_gmt']\n",
        "\n",
        "        techblogs_dict[document_url] = item\n"
      ],
      "metadata": {
        "id": "CleIVOl42nKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Chunking Using Fast API"
      ],
      "metadata": {
        "id": "_r5F91ym2ve9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lient = httpx.Client()\n",
        "\n",
        "chunking_url = \"http://chunking:5005/api/chunking\"\n",
        "\n",
        "def chunk_request(client, request_body):\n",
        "    chunking_resp = client.post(chunking_url, json=request_body, timeout=30)\n",
        "    chunks = chunking_resp.json()\n",
        "    return chunks\n",
        "\n",
        "\n",
        "# Sentence by sentence chunking\n",
        "item = example1\n",
        "\n",
        "document_title = item[\"title\"][\"rendered\"]\n",
        "document_url = item[\"link\"]\n",
        "document_html = item[\"content\"][\"rendered\"]\n",
        "document_date = item[\"date_gmt\"]\n",
        "document_date_modified = item[\"modified_gmt\"]\n",
        "\n",
        "\n",
        "chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"sentence\",\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "chunks = chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"sentence\",\n",
        "        \"chunk_min_words\": 250,\n",
        "        \"chunk_overlap_words\": 50,\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "chunks\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk #{i}\")\n",
        "    print(f\"Word Count: {sum(chunk['word_count'])}\")\n",
        "    # print(chunk[\"text\"])\n",
        "    print(\"==========\")"
      ],
      "metadata": {
        "id": "wxp59tSP2uOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chunk is this small because the default behavior of the chunking microservice is to enforce boundaries between code and non-code sections in HTML. It will not combine a section of the article written in natural language with another section of only code. This little sentence comes right before a large code section in the article, so the chunking service ends the chunk right there.\n",
        "\n",
        "Our use case therefore requires us to handle the presence of large code sections within our HTML.\n",
        "\n",
        "Lots of text documents at NVIDIA contain a mix of code and natural language, whether it's blog posts like these, SDK documentation, Git repository README markdown files, etc.\n",
        "\n",
        "These sections of code are very different syntactically and grammatically from regular natural language text, and so an embedding model that has not been trained on code may not perform well with code present. For embedding models that are trained on code and natural language, it's also going to be important to delimit the code with the characters the embedding model was trained on. The chunking service uses triple backticks (```) to indicate a section of code.\n",
        "\n",
        "The chunking service as written supports three strategies to deal with code.\n",
        "1. The default (`\"code_behavior\": \"enforce_code_boundaries\"`) is to enforce hard boundaries between code and non-code. This has the benefit of separation, but has the drawback that sometimes you will end up with awkward small chunks because of these boundaries.\n",
        "2. The second option (`\"code_behavior\": \"ignore_code_boundaries\"`) is to just ignore the boundaries and lump code and non-code together, while still keeping the backticks as delimiters. This is a good option if your embedding model supports both code and non-code.\n",
        "3. The third option (`\"code_behavior\": \"remove_code_sections\"`) is to remove the long only-code sections from the actual text that will be embedded, but store the code as metadata which can later be used. For example, the code can be supplied to an LLM that is generating a response based on the retrieval results it found by matching on the accompanying natural language."
      ],
      "metadata": {
        "id": "TTCFfLRX3RXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"sentence\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "        \"chunk_min_words\": 250,\n",
        "        \"chunk_overlap_words\": 50,\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "\n",
        "print(len(chunks))\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk #{i}\")\n",
        "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
        "    # only code. These get removed from the final text though\n",
        "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
        "    print(chunk[\"text\"])\n",
        "    print(\"==========\")\n"
      ],
      "metadata": {
        "id": "zcqWprf53P5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "item = example2\n",
        "\n",
        "document_title = item[\"title\"][\"rendered\"]\n",
        "document_url = item[\"link\"]\n",
        "document_html = item[\"content\"][\"rendered\"]\n",
        "document_date = item[\"date_gmt\"]\n",
        "document_date_modified = item[\"modified_gmt\"]\n",
        "\n",
        "\n",
        "chunks = chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"sentence\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "        \"chunk_min_words\": 250,\n",
        "        \"chunk_overlap_words\": 50,\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk #{i}\")\n",
        "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
        "    # only code. These get removed from the final text though\n",
        "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
        "    print(chunk[\"text\"])\n",
        "    print(\"==========\")"
      ],
      "metadata": {
        "id": "2f835RIX3dnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling Heading Sections:\n",
        "\n"
      ],
      "metadata": {
        "id": "BEWjWA4u3rKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item = example2\n",
        "\n",
        "document_title = item[\"title\"][\"rendered\"]\n",
        "document_url = item[\"link\"]\n",
        "document_html = item[\"content\"][\"rendered\"]\n",
        "document_date = item[\"date_gmt\"]\n",
        "document_date_modified = item[\"modified_gmt\"]\n",
        "\n",
        "\n",
        "chunks = chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"heading_section_sentence\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "        \"chunk_min_words\": 250,\n",
        "        \"chunk_overlap_words\": 50,\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk #{i}\")\n",
        "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
        "    # only code. These get removed from the final text though\n",
        "    print(f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\")\n",
        "    print(chunk[\"text\"])\n",
        "    print(\"==========\")"
      ],
      "metadata": {
        "id": "5V_f8cAi3qrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split by heading and Remove Code Sections\n",
        "\n",
        "item = example1\n",
        "\n",
        "document_title = item[\"title\"][\"rendered\"]\n",
        "document_url = item[\"link\"]\n",
        "document_html = item[\"content\"][\"rendered\"]\n",
        "document_date = item[\"date_gmt\"]\n",
        "document_date_modified = item[\"modified_gmt\"]\n",
        "\n",
        "\n",
        "chunks = chunk_request(\n",
        "    client,\n",
        "    {\n",
        "        \"strategy\": \"heading_section\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "        \"input_type\": \"html\",\n",
        "        \"input_str\": document_html,\n",
        "        \"additional_metadata\": {\n",
        "            \"document_title\": document_title,\n",
        "            \"document_url\": document_url,\n",
        "            \"document_date\": document_date,\n",
        "            \"document_date_modified\": document_date_modified,\n",
        "        },\n",
        "    },\n",
        ")\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk #{i}\")\n",
        "    # note that word_count is a list which contains word components of all text components, including ones that are\n",
        "    # only code. These get removed from the final text though\n",
        "    print(\n",
        "        f\"Word Count: {sum(wc for wc, only_code in zip(chunk['word_count'], chunk['only_code']) if not only_code)}\"\n",
        "    )\n",
        "    print(chunk[\"text\"])\n",
        "    print(\"==========\")\n",
        "\n",
        "# Concatenate non-code sections:\n",
        "clean_text_no_code = \"\\n\".join([x[\"text\"] for x in chunks])\n",
        "print(clean_text_no_code)\n",
        "\n",
        "# Concatenate code and non-code sections\n",
        "clean_text_with_code = \"\\n\".join([ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks])\n",
        "print(clean_text_with_code)"
      ],
      "metadata": {
        "id": "PaaJH7y74BM8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}