{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "For asset discovery, the chunking strategy applied here is chunking the structured data based on headings, removing non-code sections, and summarizing the non-code chunk to create the knowledge base for RAG.\n",
        "\n",
        "Model Used for Chunk Summarization: Mixtral LLM 8x7B"
      ],
      "metadata": {
        "id": "eKBnvXJ05BPL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF2qlVwS4z2O"
      },
      "outputs": [],
      "source": [
        "from llms import llms\n",
        "\n",
        "llm = llms.nim_mixtral_llm\n",
        "\n",
        "from langchain.schema.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "template = ChatPromptTemplate.from_messages(\n",
        "    [(\"user\", \"Summarize the following article in 200 words or less:\\n{user_input}\")]\n",
        ")\n",
        "\n",
        "messages = template.format_messages(\n",
        "    user_input=clean_text_no_code\n",
        ")\n",
        "\n",
        "generation: AIMessage = llm.invoke(messages)\n",
        "\n",
        "print(generation.content)"
      ],
      "metadata": {
        "id": "XkAyQfjz4_I9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the vector database by converting chunks into embeddings.\n",
        "\n",
        "We used SentenceTransformers framework with e5-large-unsupervised embedding model. In order to further icrease the infernece speed, we converted the PyTorch model to Tensor Engine file.\n",
        "\n"
      ],
      "metadata": {
        "id": "zFnwKGgf55Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import time\n",
        "import numpy as np\n",
        "import tritonclient.http\n",
        "\n",
        "triton_host = \"triton\"\n",
        "triton_port = \"8000\"\n",
        "triton_model_name = \"transformer_tensorrt_inference\"\n",
        "triton_model_version = \"1\"\n",
        "\n",
        "triton_url = f\"{triton_host}:{triton_port}\"\n",
        "\n",
        "\n",
        "def embed_with_triton(query: List[str]) -> List[List[float]]:\n",
        "    triton_client = tritonclient.http.InferenceServerClient(\n",
        "        url=triton_url, verbose=False\n",
        "    )\n",
        "\n",
        "    triton_batch_size = len(query)\n",
        "    triton_inputs = []\n",
        "    triton_outputs = []\n",
        "    triton_text_input = tritonclient.http.InferInput(\n",
        "        name=\"TEXT\", shape=(triton_batch_size,), datatype=\"BYTES\"\n",
        "    )\n",
        "    triton_text_input.set_data_from_numpy(np.asarray(query, dtype=object))\n",
        "    triton_inputs.append(triton_text_input)\n",
        "    triton_outputs.append(\n",
        "        tritonclient.http.InferRequestedOutput(\"output\", binary_data=False)\n",
        "    )\n",
        "\n",
        "    inference_results = triton_client.infer(\n",
        "        model_name=triton_model_name,\n",
        "        model_version=triton_model_version,\n",
        "        inputs=triton_inputs,\n",
        "        outputs=triton_outputs,\n",
        "    )\n",
        "\n",
        "    embedded_query = inference_results.as_numpy(\"output\").tolist()\n",
        "    return embedded_query\n",
        "\n",
        "\n",
        "embedded_query = embed_with_triton([\"query: deep learning\"])\n",
        "print(embedded_query)\n",
        "\n",
        "print(len(embedded_query))\n",
        "print(len(embedded_query[0]))"
      ],
      "metadata": {
        "id": "pYFy9PAJ9wrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redis : open source database\n",
        "\n",
        "\n",
        "We chose Redis as our database for a number of reasons.\n",
        "1. Redis is extremely fast, and we need to minimize latency for the operations it will be performing.\n",
        "2. Redis is well-supported and easy to deploy through a ready-to-go container.\n",
        "3. Redis supports both vector and keyword search: vector search through the relatively recent [RedisVL](https://github.com/RedisVentures/redisvl) project, and a fairly robust suite of [search and query features](https://redis.io/docs/interact/search-and-query/) for more traditional keyword search. Notably, Redis supports BM25, the default algorithm behind the popular Elasticsearch system--making it easy to transition smoothly between the two systems.\n",
        "4. Redis unifies our vector database with our document (and metadata) database, so we don't have to worry about maintaining keys in a separate index like [FAISS](https://faiss.ai/).\n"
      ],
      "metadata": {
        "id": "cA6hURw8-VNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%js\n",
        "var host = window.location.host;\n",
        "var url = 'http://'+host+':5006';\n",
        "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open router service API docs.</a>';\n",
        "\n",
        "%%js\n",
        "var host = window.location.host;\n",
        "var url = 'http://'+host+':5006';\n",
        "element.innerHTML = '<a style=\"color:green;\" target=\"_blank\" href='+url+'>Click to open router service API docs.</a>';"
      ],
      "metadata": {
        "id": "SCXFbyLe-cPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the asset types the router expects\n",
        "import httpx\n",
        "import json\n",
        "\n",
        "response = httpx.get(\"http://router:5006/asset-types\")\n",
        "asset_types_json = response.json()\n",
        "\n",
        "print(json.dumps(asset_types_json, indent=2))"
      ],
      "metadata": {
        "id": "M49hbdoq-uoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_dir = os.path.join(os.getcwd(), 'data', 'techblogs')\n",
        "file_list = [x for x in sorted(os.listdir(data_dir)) if \".json\" in x]\n",
        "\n",
        "payloads = []\n",
        "\n",
        "for i, filename in enumerate(file_list):\n",
        "    with open(os.path.join(data_dir, filename), \"r\") as in_file:\n",
        "        data = json.load(in_file)\n",
        "    for item in data:\n",
        "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
        "        if not item[\"link\"].startswith(\n",
        "            \"https://developer.nvidia.com/blog\"\n",
        "        ):  # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
        "            # print(f\"Skipping URL {item['link']}\")\n",
        "            continue\n",
        "        document_title = item[\"title\"][\"rendered\"]\n",
        "        document_url = item[\"link\"]\n",
        "        document_html = item[\"content\"][\"rendered\"]\n",
        "        document_date = item[\"date_gmt\"]\n",
        "        document_date_modified = item[\"modified_gmt\"]\n",
        "        payloads.append(\n",
        "            {\n",
        "                \"strategy\": \"heading_section_sentence\",\n",
        "                \"code_behavior\": \"remove_code_sections\",\n",
        "                \"chunk_min_words\": 250,\n",
        "                \"chunk_overlap_words\": 50,\n",
        "                \"input_type\": \"html\",\n",
        "                \"input_str\": document_html,\n",
        "                \"additional_metadata\": {\n",
        "                    \"document_title\": document_title,\n",
        "                    \"document_url\": document_url,\n",
        "                    \"document_date\": document_date,\n",
        "                    \"document_date_modified\": document_date_modified,\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(f\"Total num payloads: {len(payloads)}\")"
      ],
      "metadata": {
        "id": "q6ngwkS--1HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "\n",
        "chunking_url = \"http://chunking:5005/api/chunking\"\n",
        "existing_items_url = \"http://router:5006/search/keyword\"\n",
        "delete_url = \"http://router:5006/data/delete\"\n",
        "insert_url = \"http://router:5006/data/insert\"\n",
        "\n",
        "# Initialize a semaphore object with a limit of 3.\n",
        "limit = asyncio.Semaphore(3)\n",
        "\n",
        "# chunk up an article\n",
        "async def chunking_request(client: httpx.AsyncClient, payload: dict):\n",
        "    chunking_resp = await client.post(chunking_url, json=payload, timeout=15)\n",
        "    return chunking_resp.json()\n",
        "\n",
        "# see if any chunks already exist in the db that match this document url\n",
        "async def get_existing_items_request(client: httpx.AsyncClient, payload: dict, asset_type: str):\n",
        "    existing_items_resp = await client.post(\n",
        "        existing_items_url,\n",
        "        json={\n",
        "            \"field\": \"document_url\",\n",
        "            \"value\": payload[\"additional_metadata\"][\"document_url\"],\n",
        "            \"asset_types\": [asset_type],\n",
        "            \"search_type\": \"exact\",\n",
        "            \"k\": 1000,  # some large number to ensure we don't hit default limit of 10\n",
        "        },\n",
        "        timeout=15,\n",
        "    )\n",
        "    return existing_items_resp.json()\n",
        "\n",
        "# delete items with certain ids\n",
        "async def delete_request(client: httpx.AsyncClient, results: list, asset_type: str):\n",
        "    delete_resp = await client.post(\n",
        "        delete_url,\n",
        "        json={\n",
        "            \"asset_type\": asset_type,\n",
        "            \"ids\": [x[\"id\"] for x in results],\n",
        "        },\n",
        "        timeout=15,\n",
        "    )\n",
        "    print(delete_resp.status_code)\n",
        "    return delete_resp.json()"
      ],
      "metadata": {
        "id": "TCH5Y_EI-8Ft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def upload_techblogs_chunks(client: httpx.AsyncClient, payload: dict):\n",
        "    async with limit:\n",
        "        try:\n",
        "            chunks = await chunking_request(client, payload)\n",
        "        except:  # retry once\n",
        "            chunks = await chunking_request(client, payload)\n",
        "        print(\n",
        "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
        "        )\n",
        "\n",
        "        # gets ids of existing items with this url\n",
        "        try:\n",
        "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
        "        except:  # retry once\n",
        "            existing_items = await get_existing_items_request(client, payload, \"techblogs\")\n",
        "\n",
        "        if len(existing_items) > 0:\n",
        "            results = existing_items[0][\"results\"]\n",
        "            if len(results) > 0:\n",
        "                # delete items that are associated with this url\n",
        "                try:\n",
        "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
        "                except:  # retry once\n",
        "                    deleted_items = await delete_request(client, results, \"techblogs\")\n",
        "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
        "\n",
        "        # insert: send chunks to redis\n",
        "        resp = await client.post(\n",
        "            insert_url,\n",
        "            json={\n",
        "                \"asset_type\": \"techblogs\",\n",
        "                \"chunks\": chunks,\n",
        "            },\n",
        "            timeout=15,\n",
        "        )\n",
        "        print(f\"Inserted {len(resp.json())} chunks\")"
      ],
      "metadata": {
        "id": "rXuiucAR_E6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        tasks = []\n",
        "        for payload in payloads:\n",
        "            tasks.append(upload_techblogs_chunks(client, payload))\n",
        "\n",
        "        await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "atU0bQoP_HGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "# If this were not in Jupyter we would run this\n",
        "# asyncio.run(main())\n",
        "\n",
        "# Since we are in a notebook, Jupyter is already running its own event loop\n",
        "# so we can just simply await main()\n",
        "await main()\n",
        "\n",
        "end = time.perf_counter()\n",
        "\n",
        "print(f\"Took {end - start} seconds\")\n"
      ],
      "metadata": {
        "id": "doqSKRwD_LIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "techblogs_assettype = None\n",
        "\n",
        "for assettype in asset_types_json:\n",
        "    if assettype[\"name\"] ==\"techblogs\":\n",
        "        techblogs_assettype = assettype\n",
        "print(json.dumps(techblogs_assettype, indent=2))\n",
        "\n",
        "techblogs_assettype[\"chunking_params\"] = json.dumps(\n",
        "    {\n",
        "        \"strategy\": \"heading_section_sentence\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "        \"chunk_min_words\": 250,\n",
        "        \"chunk_overlap_words\": 50,\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(techblogs_assettype, indent=2))\n",
        "\n",
        "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
        "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_assettype})\n",
        "print(json.dumps(response.json(), indent=2))\n",
        "\n",
        "dump_response = httpx.post(\"http://router:5006/data/dump\")\n",
        "\n",
        "print(json.dumps(dump_response.json(), indent=2))"
      ],
      "metadata": {
        "id": "GjTqTuR7_PxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import redis\n",
        "\n",
        "r = redis.Redis(host='redis', port=6379)\n",
        "r.lastsave()"
      ],
      "metadata": {
        "id": "rVCpGj1N_bDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Programmatic Search\n"
      ],
      "metadata": {
        "id": "vkGZoiVH_pQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search_endpoint = \"http://router:5006/search/semantic\"\n",
        "\n",
        "response = httpx.post(\n",
        "    search_endpoint, json={\"query\": \"cgroups\", \"k\": 3, \"asset_types\": [\"techblogs\"]}\n",
        ")\n",
        "response.json()\n",
        "\n",
        "result1 = response.json()[0]['results'][1]\n",
        "result1\n",
        "\n",
        "heading_section_index = json.loads(result1[\"heading_section_index\"])\n",
        "heading_section_title = json.loads(result1[\"heading_section_title\"])\n",
        "paragraph_index = json.loads(result1[\"paragraph_index\"])\n",
        "contains_code = json.loads(result1[\"contains_code\"])\n",
        "only_code = json.loads(result1[\"only_code\"])\n",
        "text_components = json.loads(result1[\"text_components\"])\n",
        "\n",
        "\n",
        "assert len(heading_section_index) == len(heading_section_title) == len(paragraph_index) == len(contains_code) == len(only_code) == len(text_components)\n",
        "\n",
        "text = \"\"\n",
        "last_hsi = None\n",
        "\n",
        "for i in range(len(text_components)):\n",
        "    if last_hsi is None or last_hsi != heading_section_index[i]:\n",
        "        text += heading_section_title[i] + \"\\n\"\n",
        "    text += text_components[i]\n",
        "    if only_code[i]:\n",
        "        text += \"\\n\"\n",
        "    else:\n",
        "        text += \" \"\n",
        "    # look ahead\n",
        "    if i < len(text_components) - 1:\n",
        "        if paragraph_index[i] != paragraph_index[i+1]:\n",
        "            text += \"\\n\"\n",
        "\n",
        "    last_hsi = heading_section_index[i]\n",
        "\n",
        "print(text.strip())\n",
        "\n",
        "text = \"\"\n",
        "\n",
        "for i in range(len(text_components)):\n",
        "    if only_code[i]:\n",
        "        text += text_components[i]\n",
        "        text += \"\\n\"\n",
        "\n",
        "print(text.strip())"
      ],
      "metadata": {
        "id": "szFHkq0x_wGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Storing TechBlog Summaries in Redis\n",
        "\n",
        "from llms import llms\n",
        "llm = llms.nim_mixtral_llm\n",
        "\n",
        "payloads = []\n",
        "\n",
        "file_list = [x for x in sorted(os.listdir(data_dir)) if '.json' in x]\n",
        "\n",
        "for i, filename in enumerate(file_list):\n",
        "    with open(os.path.join(data_dir, filename), 'r') as in_file:\n",
        "        data = json.load(in_file)\n",
        "\n",
        "    for item in data:\n",
        "\n",
        "        # skip items that do not link to developer.nvidia.com/blog or blogs.nvidia.com\n",
        "        if not item['link'].startswith(\"https://developer.nvidia.com/blog\"): # and not item['link'].startswith(\"https://blogs.nvidia.com\"):\n",
        "            # print(f\"Skipping URL {item['link']}\")\n",
        "            continue\n",
        "\n",
        "        document_title = item['title']['rendered']\n",
        "        document_url = item['link']\n",
        "        document_html = item['content']['rendered']\n",
        "        document_date = item['date_gmt']\n",
        "        document_date_modified = item['modified_gmt']\n",
        "\n",
        "        payload = {\n",
        "            \"strategy\": \"heading_section\",\n",
        "            \"code_behavior\": \"remove_code_sections\",\n",
        "            \"input_type\": \"html\",\n",
        "            \"input_str\": document_html,\n",
        "            \"additional_metadata\": {\n",
        "                \"document_title\": document_title,\n",
        "                \"document_url\": document_url,\n",
        "                \"document_date\": document_date,\n",
        "                \"document_date_modified\": document_date_modified,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        payloads.append(payload)\n",
        "\n",
        "print(f\"Total num payloads: {len(payloads)}\")"
      ],
      "metadata": {
        "id": "pmjSSEBf_4QN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import httpx\n",
        "import json\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "summaries = [None] * len(payloads)\n",
        "\n",
        "# load the summaries from the json file\n",
        "with open(\"data/techblogs_summaries/saved.json\", \"r\") as f:\n",
        "    saved_summaries = json.load(f)\n",
        "\n",
        "# Initialize a semaphore object with a limit of 3.\n",
        "limit = asyncio.Semaphore(3)\n",
        "\n",
        "async def async_generate(llm, msg):\n",
        "    resp = await llm.agenerate([msg])\n",
        "    return resp.generations[0][0].text\n",
        ""
      ],
      "metadata": {
        "id": "REVN_GcSAGIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment the following line if we want to save new summaries.\n",
        "# saved_summaries = {}\n",
        "\n",
        "async def upload_techblogs_summaries(llm, client: httpx.AsyncClient, payload: dict):\n",
        "    async with limit:\n",
        "\n",
        "        try:\n",
        "            chunks = await chunking_request(client, payload)\n",
        "        except:  # retry once\n",
        "            chunks = await chunking_request(client, payload)\n",
        "        print(\n",
        "            f\"{payload['additional_metadata']['document_url']} | num chunks: {len(chunks)}\"\n",
        "        )\n",
        "\n",
        "        clean_text_no_code = \"\\n\".join([x[\"text\"] for x in chunks])\n",
        "        clean_text_with_code = \"\\n\".join([ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks])\n",
        "\n",
        "        # Ask LLM for summaries\n",
        "\n",
        "        # uncomment if we want to save new summaries\n",
        "        # template = ChatPromptTemplate.from_messages(\n",
        "        #     [(\"user\", \"Summarize the following article in 200 words or less:\\n{user_input}\")]\n",
        "        # )\n",
        "\n",
        "        # msg = template.format_messages(\n",
        "        #     user_input=clean_text_no_code\n",
        "        # )\n",
        "\n",
        "        # summary = await async_generate(llm, msg)\n",
        "        # summary_with_metadata = [\n",
        "        #     {\n",
        "        #         \"text\": payload[\"additional_metadata\"][\"document_title\"] + \"\\n\" + summary,\n",
        "        #         \"text_components\": [ x[\"heading_section_title\"][0] + \"\\n\" + \"\\n\".join(x[\"text_components\"]) for x in chunks],\n",
        "        #         \"document_title\": payload[\"additional_metadata\"][\"document_title\"],\n",
        "        #         \"document_url\": payload[\"additional_metadata\"][\"document_url\"],\n",
        "        #         \"document_date\": payload[\"additional_metadata\"][\"document_date\"],\n",
        "        #         \"document_date_modified\": payload[\"additional_metadata\"][\"document_date_modified\"],\n",
        "        #         \"document_full_text\": clean_text_with_code\n",
        "        #     }\n",
        "        # ]\n",
        "        # saved_summaries[payload[\"additional_metadata\"][\"document_url\"]] = summary_with_metadata\n",
        "\n",
        "        # load summary we've already generated\n",
        "        # comment the following line if we want to save new summaries\n",
        "        summary_with_metadata = saved_summaries[payload[\"additional_metadata\"][\"document_url\"]]\n",
        "\n",
        "        # gets ids of existing items with this url\n",
        "        try:\n",
        "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
        "        except:  # retry once\n",
        "            existing_items = await get_existing_items_request(client, payload, \"summarize_techblogs\")\n",
        "\n",
        "        if len(existing_items) > 0:\n",
        "            results = existing_items[0][\"results\"]\n",
        "            if len(results) > 0:\n",
        "                # delete items that are associated with this url\n",
        "                try:\n",
        "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
        "                except:  # retry once\n",
        "                    deleted_items = await delete_request(client, results, \"summarize_techblogs\")\n",
        "                print(f\"Deleted ids reponse: {deleted_items}\")\n",
        "\n",
        "        # insert: send chunks to redis\n",
        "        resp = await client.post(\n",
        "            insert_url,\n",
        "            json={\n",
        "                \"asset_type\": \"summarize_techblogs\",\n",
        "                \"chunks\": summary_with_metadata,\n",
        "            },\n",
        "            timeout=15,\n",
        "        )\n",
        "        print(f\"Inserted {len(resp.json())} chunks\")"
      ],
      "metadata": {
        "id": "G0FSNIvYAPCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    async with httpx.AsyncClient() as client:\n",
        "        tasks = []\n",
        "        # for i in range(0, 7):\n",
        "        for i in range(0, len(payloads)):\n",
        "            tasks.append(upload_techblogs_summaries(llm, client, payloads[i]))\n",
        "\n",
        "        await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "wqImKFkzAQw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter()\n",
        "\n",
        "# If this were not in Jupyter we would run this\n",
        "# asyncio.run(main())\n",
        "\n",
        "# Since we are in a notebook, Jupyter is already running its own event loop\n",
        "# so we can just simply await main()\n",
        "await main()\n",
        "\n",
        "end = time.perf_counter()\n",
        "\n",
        "print(f\"Took {end - start} seconds\")\n",
        "\n",
        "# This should take around 2-3 minutes"
      ],
      "metadata": {
        "id": "sRx36nMhASFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the summaries as a json file\n",
        "with open(\"data/techblogs_summaries/saved.json\", \"w\") as f:\n",
        "    json.dump(saved_summaries, f)"
      ],
      "metadata": {
        "id": "vU3J00mpATrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "techblogs_summaries_assettype = None\n",
        "\n",
        "for assettype in asset_types_json:\n",
        "    if assettype[\"name\"] ==\"summarize_techblogs\":\n",
        "        techblogs_summaries_assettype = assettype\n",
        "\n",
        "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
        "\n",
        "techblogs_summaries_assettype[\"chunking_params\"] = json.dumps(\n",
        "    {\n",
        "        \"strategy\": \"summarization\",\n",
        "        \"code_behavior\": \"remove_code_sections\",\n",
        "    }\n",
        ")\n",
        "\n",
        "print(json.dumps(techblogs_summaries_assettype, indent=2))\n",
        "\n",
        "\n",
        "update_asset_types_url = \"http://router:5006/asset-types/update\"\n",
        "response = httpx.post(update_asset_types_url, json={\"data\": techblogs_summaries_assettype})\n",
        "print(json.dumps(response.json(), indent=2))\n",
        "\n",
        "dump_response = httpx.post(\"http://router:5006/data/dump\")\n",
        "print(json.dumps(dump_response.json(), indent=2))"
      ],
      "metadata": {
        "id": "JCsG4P_aAVDr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}